{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3Jd9zq8s-H2"
   },
   "source": [
    "# SSC300: A fast model to count multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDO-cs1ls-H3"
   },
   "source": [
    "In this notebook it is shown a comparison of the serveral variants of the Keras implementation of the SSC model. The notebook can be ran on Google Colab to have a free Tesla K80 12GB GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArFU64EcW8eT"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03EMx5tCW_H-"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYB-R7-4wvFV"
   },
   "source": [
    "## Download of datasets and Packages decompression\n",
    "This series of cells allows to download and uncompress the PASCAL VOC 2007 and 2012 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t36YLsJr9m9E"
   },
   "outputs": [],
   "source": [
    "!rm -r object_counting object_classification utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2V0FHlStX3c"
   },
   "outputs": [],
   "source": [
    "!tar -xf keras.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndGKW8p1MI2"
   },
   "outputs": [],
   "source": [
    "!rm -r weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LNT2d314tEp"
   },
   "outputs": [],
   "source": [
    "!mkdir weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--WAMJSMGp--"
   },
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_W6IJMiGvX3"
   },
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-vot-TFyZLC"
   },
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTS53n0gHnX7"
   },
   "outputs": [],
   "source": [
    "!tar -xf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfz3mw7pHa4x"
   },
   "outputs": [],
   "source": [
    "!tar -xf VOCtrainval_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlqjEbqPybeA"
   },
   "outputs": [],
   "source": [
    "!tar -xf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "32kNyhJdcWKH"
   },
   "outputs": [],
   "source": [
    "!python -m  pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ysw4tXvs-H6"
   },
   "source": [
    "## Models applied to PASCAL VOC dataset\n",
    "\n",
    "From now on, I will show you how to use the code and instantiate the different models and test them on the PASCAL VOC dataset.\n",
    "\n",
    "In each block, most of the operations are the same thanks to the flexibility of the class inspired by Object Oriented paradigm, exploiting objects ineritance and interface style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdtygPVos-H7"
   },
   "source": [
    "### Preliminary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCDucc0Bs-H8"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import poisson\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "from object_counting.keras_ssc.misc_utils.tensor_sampling_utils import sample_tensors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from object_counting.keras_ssc.models.keras_ssc300_4_merged import ssc_300 as ssc_300_m\n",
    "from object_counting.keras_ssc.models.keras_ssc300 import ssc_300 as ssc_300_normal\n",
    "from object_counting.keras_ssc.keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from object_counting.keras_ssc.keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from object_counting.keras_ssc.ssd_encoder_decoder.ssc_input_encoder import SSCInputEncoder\n",
    "\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from object_counting.keras_ssc.data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwPb_tYAs-IB"
   },
   "source": [
    "### Setting variables\n",
    "\n",
    "For more info about the variables check the doumentation of the package. If it is not build, you can use sphynx to generate the html docs from the code doc strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HenJLUFas-IC"
   },
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_pascal = [0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "aspect_ratios = [[1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]]\n",
    "hidden_sizes = [250, 250, 100]\n",
    "predictors = ['conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "steps = [32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtShzXhbs-IH"
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2yIKSTjRKwf"
   },
   "source": [
    "### Implement Models\n",
    "After having defined all the parameters, we instantiate the model, instantiate an optimizer, maybe load weights and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBdJ0ZvzRJW-"
   },
   "outputs": [],
   "source": [
    "K.clear_session() # Clear previous models from memory\n",
    "\n",
    "model_4 = ssc_300_normal(image_size=(img_height, img_width, img_channels),\n",
    "                         n_classes=n_classes,\n",
    "                         mode='training',\n",
    "                         l2_regularization=0.0005,\n",
    "                         scales=scales,\n",
    "                         aspect_ratios_per_layer=aspect_ratios,\n",
    "                         two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                         steps=steps,\n",
    "                         offsets=offsets,\n",
    "                         variances=variances,\n",
    "                         subtract_mean=mean_color,\n",
    "                         swap_channels=swap_channels,\n",
    "                         predictors=predictors,\n",
    "                         output_activation=True,\n",
    "                         lstm=False,\n",
    "                         condense_predictors=False)\n",
    "\n",
    "model_m = ssc_300_m(image_size=(img_height, img_width, img_channels),\n",
    "                    n_classes=n_classes,\n",
    "                    mode='training',\n",
    "                    l2_regularization=0.0005,\n",
    "                    scales=scales,\n",
    "                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                    steps=steps,\n",
    "                    offsets=offsets,\n",
    "                    variances=variances,\n",
    "                    subtract_mean=mean_color,\n",
    "                    swap_channels=swap_channels,\n",
    "                    output_activation=True,\n",
    "                    lstm=False,\n",
    "                    condense_predictors=False)\n",
    "\n",
    "model_m_c = ssc_300_m(image_size=(img_height, img_width, img_channels),\n",
    "                      n_classes=n_classes,\n",
    "                      mode='training',\n",
    "                      l2_regularization=0.0005,\n",
    "                      scales=scales,\n",
    "                      aspect_ratios_per_layer=aspect_ratios,\n",
    "                      two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                      steps=steps,\n",
    "                      offsets=offsets,\n",
    "                      variances=variances,\n",
    "                      subtract_mean=mean_color,\n",
    "                      swap_channels=swap_channels,\n",
    "                      output_activation=True,\n",
    "                      lstm=False,\n",
    "                      condense_predictors=True)\n",
    "\n",
    "model_m_lstmc = ssc_300_m(image_size=(img_height, img_width, img_channels),\n",
    "                          n_classes=n_classes,\n",
    "                          mode='training',\n",
    "                          l2_regularization=0.0005,\n",
    "                          scales=scales,\n",
    "                          aspect_ratios_per_layer=aspect_ratios,\n",
    "                          two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                          steps=steps,\n",
    "                          offsets=offsets,\n",
    "                          variances=variances,\n",
    "                          subtract_mean=mean_color,\n",
    "                          swap_channels=swap_channels,\n",
    "                          output_activation=True,\n",
    "                          lstm=True,\n",
    "                          condense_predictors=True)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "\n",
    "\n",
    "# If by_name is set to False, the structure must be identical,\n",
    "# while if set to True, only the layers with the same name must be identical\n",
    "print(\"LOADING WEIGHTS...\\n\")\n",
    "model_4.load_weights('gdrive/My Drive/ColabFiles/ssc300_basic_VOC_07+12.h5')\n",
    "model_m.load_weights('gdrive/My Drive/ColabFiles/ssc300_m_VOC_07+12.h5')\n",
    "model_m_c.load_weights('gdrive/My Drive/ColabFiles/ssc300_m_c_VOC_07+12.h5')\n",
    "model_m_lstmc.load_weights('gdrive/My Drive/ColabFiles/ssc300_m_lstm_c_ft_VOC_07+12.h5')\n",
    "\n",
    "models = [model_4, model_m, model_m_c, model_m_lstmc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1F5WNhTPTzza"
   },
   "source": [
    "### Load datasets and convert them in the correct format for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLEmPImus-IR"
   },
   "outputs": [],
   "source": [
    "# 1: Instantiate `DataGenerator` objects: One for training, one for validation.\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "test_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "# TODO: Set the paths to the datasets here.\n",
    "\n",
    "# The directories that contain the images.\n",
    "images_dir      = '/content/VOCdevkit/VOC2007/JPEGImages/'\n",
    "images_dir_12   = '/content/VOCdevkit/VOC2012/JPEGImages/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "annotations_dir      = '/content/VOCdevkit/VOC2007/Annotations/'\n",
    "annotations_dir_12   = '/content/VOCdevkit/VOC2012/Annotations/'\n",
    "\n",
    "# The paths to the image sets.\n",
    "train_image_set_filename    = '/content/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
    "train_image_set_filename_12    = '/content/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt'\n",
    "val_image_set_filename      = '/content/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\n",
    "test_image_set_filename     = '/content/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[images_dir, images_dir_12],\n",
    "                        image_set_filenames=[train_image_set_filename, train_image_set_filename_12],\n",
    "                        annotations_dirs=[annotations_dir, annotations_dir_12],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[images_dir],\n",
    "                      image_set_filenames=[val_image_set_filename],\n",
    "                      annotations_dirs=[annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "\n",
    "test_dataset.parse_xml(images_dirs=[images_dir],\n",
    "                      image_set_filenames=[test_image_set_filename],\n",
    "                      annotations_dirs=[annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training. Doing this is not relevant in case you activated the `load_images_into_memory`\n",
    "# option in the constructor, because in that cas the images are in memory already anyway. If you want \n",
    "# to create HDF5 datasets, uncomment the subsequent two function calls.\n",
    "\n",
    "# train_dataset.create_hdf5_dataset(file_path='/content/VOCdevkit/VOC2007/voc_train.h5',\n",
    "#                                   resize=False,\n",
    "#                                   variable_image_size=True,\n",
    "#                                   verbose=True)\n",
    "\n",
    "# val_dataset.create_hdf5_dataset(file_path='/content/VOCdevkit/VOC2007/voc_val.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)\n",
    "\n",
    "# test_dataset.create_hdf5_dataset(file_path='/content/VOCdevkit/VOC2007/voc_test.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJF1Nk9vDW0a"
   },
   "outputs": [],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 1 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [# model.get_layer('conv4_3_norm').output_shape[1:3],\n",
    "                   # model.get_layer('fc7').output_shape[1:3],\n",
    "                   model_4.get_layer('conv6_2').output_shape[1:3],\n",
    "                   model_4.get_layer('conv7_2').output_shape[1:3],\n",
    "                   model_4.get_layer('conv8_2').output_shape[1:3],\n",
    "                   model_4.get_layer('conv9_2').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSCInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "from object_counting.keras_ssc.ssd_encoder_decoder.ssc_input_encoder_1pred import SSCInputEncoder1Pred\n",
    "\n",
    "ssc_input_encoder = SSCInputEncoder1Pred(len(classes))\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                          resize],\n",
    "                                         label_encoder=ssc_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssc_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "test_generator = test_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssc_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "test_dataset_size   = test_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))\n",
    "print(\"Number of images in the test dataset:\\t\\t{:>6}\".format(test_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDXkRf5-vHuc"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6360eifYvC3V"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "predictions = []\n",
    "gts = []\n",
    "imgs = []\n",
    "labels = []\n",
    "times = np.zeros(len(models))\n",
    "\n",
    "for m in models:\n",
    "    predictions.append(list())\n",
    "\n",
    "for i in tqdm(range(ceil(test_dataset_size/batch_size))):\n",
    "    batch_x, batch_y = next(test_generator)\n",
    "    imgs.extend(batch_x)\n",
    "    gts.extend(batch_y)\n",
    "    \n",
    "    for j, m in enumerate(models):\n",
    "        time_temp = time()\n",
    "        predictions[j].extend(m.predict(batch_x, batch_size))\n",
    "        times[j] += time() - time_temp\n",
    "\n",
    "imgs = np.array(imgs)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i]= np.array(predictions[i])\n",
    "    if i < 2:\n",
    "        predictions[i] = np.reshape(predictions[i], (-1,len(classes),len(predictor_sizes)), order='F').sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAx7RktVc5TV"
   },
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "gts = np.array(gts)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics of the 4 models\n",
    "Here you can see the metrics computed per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRnFFgBgd8bI"
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "\n",
    "def pretty_prediction_multimodel(classes, y_pred, y_true, pred_idx=0):\n",
    "    headers = ['class', 'g_truth', 'basic 4', 'merged', 'm_cond', 'm_lstm_c']\n",
    "    table = []\n",
    "    \n",
    "    for i in range(y_pred.shape[1]):\n",
    "        print(\"Prediction {}\".format(i))\n",
    "        for c in range(len(classes)):\n",
    "            if not np.round(y_pred[:,i,c]).sum() == 0 or not y_true[i,c] == 0:\n",
    "                row = [classes[c], y_true[i,c]]\n",
    "                for m in range(y_pred.shape[0]):\n",
    "                    row.append(np.round(y_pred[m, i, c]))\n",
    "            table.append(row)  \n",
    "    print(tabulate(table, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuZMufj_vRua"
   },
   "outputs": [],
   "source": [
    "model_names = ['ssc-basic', 'ssc-m', 'ssc-m-c', 'ssc-m-lstm-c']\n",
    "headers_rmse = ['RMSE', 'mRMSE', 'm_relRMSE']\n",
    "table_rmse = []\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    RMSE = np.sqrt(np.mean((p-gts)**2))\n",
    "    mRMSE = np.mean(np.sqrt(np.mean((p-gts)**2, axis=0)))\n",
    "    m_relRMSE = np.mean(np.sqrt(np.mean(((p-gts)**2)/(gts+1), axis=0)))\n",
    "    table_rmse.append([model_names[i], RMSE, mRMSE, m_relRMSE])\n",
    "\n",
    "print(tabulate(table_rmse, headers=headers_rmse))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some nice plots\n",
    "The following generated plots represent the error of the models in relation to the count, and subsequently, the percentage of correctly, overly and underly estimated counts per number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQS6ag1gpZw4"
   },
   "outputs": [],
   "source": [
    "gts_totnum = gts.sum(axis=1)\n",
    "\n",
    "model_names = ['ssc-basic', 'ssc-m', 'ssc-m-c', 'ssc-m-lstm-c']\n",
    "headers_num = ['Model', '# Objects', 'mRMSE']\n",
    "table_num = []\n",
    "\n",
    "for i in range(15):\n",
    "    mask = gts_totnum == i + 1\n",
    "    for j, p in enumerate(predictions):\n",
    "        mRMSE = np.mean(np.sqrt(np.mean((p[mask,:]-gts[mask, :])**2, axis=0)))\n",
    "        table_num.append([model_names[j], i + 1, mRMSE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJKNqRW1wHNi"
   },
   "outputs": [],
   "source": [
    "mRMSE_num = []\n",
    "\n",
    "for i in range(15):\n",
    "    a = []\n",
    "    for j in range(predictions.shape[0]):\n",
    "        a.append(table_num[i*4+j][2])\n",
    "        mRMSE_num.append(a)\n",
    "\n",
    "    mRMSE_num = np.array(mRMSE_num).T\n",
    "    mRMSE_num = {model_names[i]:mRMSE_num[i] for i in range(mRMSE_num.shape[0])}\n",
    "    mRMSE_num['x'] = range(1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eIQwjjDF8YVE"
   },
   "outputs": [],
   "source": [
    "colors = ['coral', 'goldenrod', 'mediumseagreen', 'steelblue']\n",
    "plt.rcParams[\"figure.figsize\"] = [8.0, 5.0]\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.plot( 'x', model_names[i], data=mRMSE_num, marker='o', color=colors[i], linewidth=2, label=model_names[i])\n",
    "plt.legend()\n",
    "plt.xticks(mRMSE_num['x'])\n",
    "plt.ylabel('Count Error (mRMSE)')\n",
    "plt.xlabel('Counts')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szo81Q9VHubA"
   },
   "outputs": [],
   "source": [
    "gts_totnum_c = gts\n",
    "\n",
    "model_names = ['ssc-basic', 'ssc-m', 'ssc-m-c', 'ssc-m-lstm-c']\n",
    "table_num_c = []\n",
    "\n",
    "for i in range(16):\n",
    "    mRMSE = np.zeros((predictions.shape[0], len(classes)))\n",
    "    for j, p in enumerate(predictions):\n",
    "        for c in range(len(classes)):\n",
    "            mask = gts_totnum_c[:, c] == i\n",
    "            if (p[mask,c]-gts[mask, c]).shape[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                mRMSE[j, c] += np.sqrt(np.mean((p[mask,c]-gts[mask, c])**2))\n",
    "            table_num_c.append([model_names[j], i + 1, np.true_divide(mRMSE[j].sum(),(mRMSE[j]!=0).sum())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPrU3XOjLH12"
   },
   "outputs": [],
   "source": [
    "mRMSE_num_c = []\n",
    "\n",
    "for i in range(16):\n",
    "    a = []\n",
    "    for j in range(predictions.shape[0]):\n",
    "        a.append(table_num_c[i*4+j][2])\n",
    "        mRMSE_num_c.append(a)\n",
    "\n",
    "mRMSE_num_c = np.array(mRMSE_num_c).T\n",
    "mRMSE_num_c = {model_names[i]:mRMSE_num_c[i] for i in range(mRMSE_num_c.shape[0])}\n",
    "mRMSE_num_c['x'] = range(0,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tausxBiMLTCd"
   },
   "outputs": [],
   "source": [
    "colors = ['coral', 'goldenrod', 'mediumseagreen', 'steelblue']\n",
    "plt.rcParams[\"figure.figsize\"] = [8.0, 5.0]\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.plot( 'x', model_names[i], data=mRMSE_num_c, marker='o', color=colors[i], linewidth=2, label=model_names[i])\n",
    "plt.legend()\n",
    "plt.xticks(mRMSE_num_c['x'])\n",
    "plt.ylabel('Count Error (mRMSE)')\n",
    "plt.xlabel('Counts')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WMDdtqXYdhb"
   },
   "outputs": [],
   "source": [
    "count_c = np.zeros((16, len(classes)))\n",
    "count = np.zeros(16)\n",
    "for i in range(16):\n",
    "    for c in range(len(classes)):\n",
    "        count_c[i, c] += ((gts[gts[:, c] == i, c]) != 0).sum()\n",
    "    count[i] = ((gts.sum(axis=1) == i) != 0).sum()\n",
    "\n",
    "print(count_c.mean(axis=1))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LkeIksVseczJ"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(1,16), count_c.mean(axis=1)[1:])\n",
    "plt.xticks(range(1, 16))\n",
    "plt.ylabel('Average # of Images per Class per Count')\n",
    "plt.xlabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "axgInztrfxSp"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(1,16), count[1:])\n",
    "plt.xticks(range(1, 16))\n",
    "plt.ylabel('# of Images per Count')\n",
    "plt.xlabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "se5HykxdjkKT"
   },
   "outputs": [],
   "source": [
    "under = []\n",
    "over = []\n",
    "correct = []\n",
    "\n",
    "for p in predictions:\n",
    "    under.append(np.round(p) < gts)\n",
    "    over.append(np.round(p) > gts)\n",
    "    correct.append(np.round(p) == gts)\n",
    "\n",
    "    \n",
    "under = np.array(under)\n",
    "over = np.array(over)\n",
    "correct = np.array(correct)\n",
    "\n",
    "print(under.shape)\n",
    "\n",
    "totals = under.sum(axis=1) + over.sum(axis=1)  + correct.sum(axis=1) \n",
    "\n",
    "print(\"Underestimate: \", under.sum(axis=1)  / totals)\n",
    "print(\"Overestimate: \", over.sum(axis=1)  / totals)\n",
    "print(\"Correct: \", correct.sum(axis=1)  / totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLUHzHc1mflP"
   },
   "outputs": [],
   "source": [
    "percentage = np.zeros((predictions.shape[0], 15, len(classes), 3))\n",
    "for i in range(1, 16):\n",
    "    for j, p in enumerate(predictions):\n",
    "        for c in range(len(classes)):\n",
    "            mask = gts_totnum == i\n",
    "            mask2 = np.logical_and(gts[:, c] + np.round(p[:, c]) > 0.0, mask)\n",
    "            if gts[mask2, c].shape[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                total = under[j, mask2, c].sum() + over[j, mask2, c].sum() + correct[j, mask2, c].sum()\n",
    "                percentage[j, i-1, c, 0] = under[j, mask2, c].sum() / total\n",
    "                percentage[j, i-1, c, 1] = over[j, mask2,c ].sum() / total\n",
    "                percentage[j, i-1, c, 2] = correct[j, mask2, c].sum() / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnZWJIf4Keoo"
   },
   "outputs": [],
   "source": [
    "percentage_total = np.zeros((predictions.shape[0], 15, 3))\n",
    "for i in range(1, 16):\n",
    "    for j, p in enumerate(predictions):\n",
    "        total = 0\n",
    "        for c in range(len(classes)):\n",
    "        mask = gts_totnum == i\n",
    "        mask2 = np.logical_and(gts[:, c] + np.round(p[:, c]) > 0.0, mask)\n",
    "        if gts[mask2, c].shape[0] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            total += under[j, mask2, c].sum() + over[j, mask2, c].sum() + correct[j, mask2, c].sum()\n",
    "            percentage_total[j, i-1, 0] += under[j, mask2, c].sum()\n",
    "            percentage_total[j, i-1, 1] += over[j, mask2,c ].sum()\n",
    "            percentage_total[j, i-1, 2] += correct[j, mask2, c].sum()\n",
    "        percentage_total[j, i-1, :] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xq37oY7bD8Ku"
   },
   "outputs": [],
   "source": [
    "percentage_mean = percentage.sum(axis=2) / np.expand_dims(percentage.sum(axis=2).sum(axis=2), axis=2)\n",
    "percentage_mean_global = percentage.sum(axis=1).sum(axis=1) / np.expand_dims(percentage.sum(axis=1).sum(axis=1).sum(axis=1), axis=1)\n",
    "print(percentage_mean_global.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPDKUC7etFfc"
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax.bar(np.array(range(0, 4)) - 0.3, percentage_mean_global[:,2], width=0.3, color='g', align='center', label='correct')\n",
    "ax.bar(np.array(range(0, 4)), percentage_mean_global[:,0], width=0.3, color='b', align='center', label='under-estimated')\n",
    "ax.bar(np.array(range(0, 4)) + 0.3, percentage_mean_global[:, 1], width=0.3, color='r', align='center', label='over-estimated')\n",
    "\n",
    "plt.xticks(range(4), labels=['ssc-basic', 'ssc-m', 'ssc-m-c', 'ssc-m-lstm-c'])\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Models')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "  \n",
    "for p in percentage_total:\n",
    "    ax = plt.subplot()\n",
    "    ax.bar(np.array(range(1, 16)) - 0.3, p[:, 2], width=0.3, color='g', align='center', label='correct')\n",
    "    ax.bar(np.array(range(1, 16)), p[:, 0], width=0.3, color='b', align='center', label='under-estimated')\n",
    "    ax.bar(np.array(range(1, 16)) + 0.3, p[:, 1], width=0.3, color='r', align='center', label='over-estimated')\n",
    "\n",
    "    plt.xticks(range(1, 16))\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xlabel('Counts')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-yf4fotiimu"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, 4952)\n",
    "# 4469 plane and reflected people\n",
    "# 2106 bottles\n",
    "\n",
    "imshow(imgs[i])\n",
    "print(i)\n",
    "pretty_prediction_multimodel(classes, predictions[:, i:i+1], gts[i:i+1], i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SSC300 Evaluation.ipynb",
   "provenance": [
    {
     "file_id": "158JozYm6Fd-twoFdRaxXPVHeUOTd7fvn",
     "timestamp": 1558354227632
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
