{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3Jd9zq8s-H2"
   },
   "source": [
    "# SSC300: A fast model to count multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDO-cs1ls-H3"
   },
   "source": [
    "In this notebook it is shown a comparison of the serveral variants of the Keras implementation of the SSC model. The notebook can be ran on Google Colab to have a free Tesla K80 12GB GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArFU64EcW8eT"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03EMx5tCW_H-"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYB-R7-4wvFV"
   },
   "source": [
    "## Download of datasets and Packages decompression\n",
    "This series of cells allows to download and uncompress the PASCAL VOC 2007 and 2012 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t36YLsJr9m9E"
   },
   "outputs": [],
   "source": [
    "!rm -r object_counting object_classification utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2V0FHlStX3c"
   },
   "outputs": [],
   "source": [
    "!tar -xf keras.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kndGKW8p1MI2"
   },
   "outputs": [],
   "source": [
    "!rm -r weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LNT2d314tEp"
   },
   "outputs": [],
   "source": [
    "!mkdir weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--WAMJSMGp--"
   },
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_W6IJMiGvX3"
   },
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-vot-TFyZLC"
   },
   "outputs": [],
   "source": [
    "!wget http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTS53n0gHnX7"
   },
   "outputs": [],
   "source": [
    "!tar -xf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfz3mw7pHa4x"
   },
   "outputs": [],
   "source": [
    "!tar -xf VOCtrainval_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlqjEbqPybeA"
   },
   "outputs": [],
   "source": [
    "!tar -xf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ysw4tXvs-H6"
   },
   "source": [
    "## Models applied to PASCAL VOC dataset\n",
    "\n",
    "From now on, I will show you how to use the code and instantiate the different models and test them on the PASCAL VOC dataset.\n",
    "\n",
    "In each block, most of the operations are the same thanks to the flexibility of the class inspired by Object Oriented paradigm, exploiting objects ineritance and interface style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdtygPVos-H7"
   },
   "source": [
    "### Preliminary imports\n",
    "All the necessary packages are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCDucc0Bs-H8"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import poisson\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import h5py\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "from object_counting.keras_ssc.misc_utils.tensor_sampling_utils import sample_tensors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from object_counting.keras_ssc.models.keras_ssc300_4_merged import ssc_300\n",
    "from object_counting.keras_ssc.keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from object_counting.keras_ssc.keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from object_counting.keras_ssc.ssc_encoder_decoder.ssc_input_encoder import SSCInputEncoder\n",
    "from object_counting.keras_ssc.ssc_encoder_decoder.ssc_input_encoder_1pred import SSCInputEncoder1Pred\n",
    "\n",
    "\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from object_counting.keras_ssc.data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from object_counting.keras_ssc.data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwPb_tYAs-IB"
   },
   "source": [
    "### Setting variables\n",
    "\n",
    "For more info about the variables check the doumentation of the package. If it is not build, you can use sphynx to generate the html docs from the code doc strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HenJLUFas-IC"
   },
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_pascal = [0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "aspect_ratios = [[1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]]\n",
    "hidden_sizes = [250, 250, 100]\n",
    "predictors = ['conv6_2', 'conv7_2', 'conv8_2', 'conv9_2']\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "steps = [32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dtShzXhbs-IH"
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbALg9jY17tF"
   },
   "source": [
    "### Implement the Model\n",
    "After having defined all the parameters, we instantiate the model, instantiate an optimizer, maybe load weights and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt9n5r_Q2C1s"
   },
   "outputs": [],
   "source": [
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "#    Instantiate an optimizer and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, clipvalue=1.)\n",
    "#sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "model = ssc_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels,\n",
    "                output_activation=True,\n",
    "                lstm=True,\n",
    "                condense_predictors=True)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "weights_path = 'gdrive/My Drive/ColabFiles/ssc300_4_m_lstm_c_ftall.h5'\n",
    "\n",
    "# If by_name is set to False, the structure must be identical,\n",
    "# while if set to True, only the layers with the same name must be identical\n",
    "print(\"LOADING WEIGHTS...\\n\")\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "print(\"COMPILE THE MODEL\\n\")\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mWI5v4S_KFkV"
   },
   "outputs": [],
   "source": [
    "for i, l in enumerate(model.layers):\n",
    "    print(i, l.name, l.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SJCYMkks-IL"
   },
   "source": [
    "### Load datasets and convert them in the correct format for the model\n",
    "Instantiate one DataGenerator object for each set you want to load. If the hdf5 path is et it will load the dataset from the H5 Dataset. Loading into memory is faster but is only possible if there is enough free space in teh memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLEmPImus-IR"
   },
   "outputs": [],
   "source": [
    "# 1: Instantiate `DataGenerator` objects: One for training, one for validation.\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "test_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "# TODO: Set the paths to the datasets here.\n",
    "\n",
    "# The directories that contain the images.\n",
    "images_dir      = '/content/VOCdevkit/VOC2007/JPEGImages/'\n",
    "images_dir_12   = '/content/VOCdevkit/VOC2012/JPEGImages/'\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "annotations_dir      = '/content/VOCdevkit/VOC2007/Annotations/'\n",
    "annotations_dir_12   = '/content/VOCdevkit/VOC2012/Annotations/'\n",
    "\n",
    "# The paths to the image sets.\n",
    "train_image_set_filename    = '/content/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
    "train_image_set_filename_12    = '/content/VOCdevkit/VOC2012/ImageSets/Main/trainval.txt'\n",
    "val_image_set_filename      = '/content/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\n",
    "test_image_set_filename     = '/content/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "# Parse the sets\n",
    "train_dataset.parse_xml(images_dirs=[images_dir, images_dir_12],\n",
    "                        image_set_filenames=[train_image_set_filename, train_image_set_filename_12],\n",
    "                        annotations_dirs=[annotations_dir, annotations_dir_12],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "val_dataset.parse_xml(images_dirs=[images_dir],\n",
    "                      image_set_filenames=[val_image_set_filename],\n",
    "                      annotations_dirs=[annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "\n",
    "test_dataset.parse_xml(images_dirs=[images_dir],\n",
    "                      image_set_filenames=[test_image_set_filename],\n",
    "                      annotations_dirs=[annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "\n",
    "# Optional: Convert the dataset into an HDF5 dataset. This will require more disk space, but will\n",
    "# speed up the training if you don't load the images in memory. Doing this is not relevant in case \n",
    "# you activated the `load_images_into_memory` option in the constructor, \n",
    "# because in that case the images are in memory already anyway. If you want \n",
    "# to create HDF5 datasets, uncomment the subsequent two function calls.\n",
    "\n",
    "# train_dataset.create_hdf5_dataset(file_path='/content/VOCdevkit/VOC2007/voc_train.h5',\n",
    "#                                   resize=False,\n",
    "#                                   variable_image_size=True,\n",
    "#                                   verbose=True)\n",
    "\n",
    "# val_dataset.create_hdf5_dataset(file_path='/content/VOCdevkit/VOC2007/voc_val.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)\n",
    "\n",
    "# test_dataset.create_hdf5_dataset(file_path='/content/VOCdevkit/VOC2007/voc_test.h5',\n",
    "#                                 resize=False,\n",
    "#                                 variable_image_size=True,\n",
    "#                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJF1Nk9vDW0a"
   },
   "outputs": [],
   "source": [
    "# 3: Set the batch size.\n",
    "batch_size = 25 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [# model.get_layer('conv4_3_norm').output_shape[1:3],\n",
    "                   # model.get_layer('fc7').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSCInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "\n",
    "ssc_input_encoder = SSCInputEncoder1Pred(len(classes))\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[convert_to_3_channels,\n",
    "                                                          resize],\n",
    "                                         label_encoder=ssc_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssc_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "test_generator = test_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssc_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "test_dataset_size   = test_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))\n",
    "print(\"Number of images in the test dataset:\\t\\t{:>6}\".format(test_dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24skOBIrDc0C"
   },
   "source": [
    " ### Train the model\n",
    " If desired, define a learning rate schedule, then create the callbacks necessary to save the weights and set the learing rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ULhTbjXDlDv"
   },
   "outputs": [],
   "source": [
    "# Define a learning rate schedule.\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 20:\n",
    "        return 0.001\n",
    "    elif epoch < 80:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHfUHSglDnSL"
   },
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='weights/ssc300_pascal_07+12_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "#model_checkpoint.best = \n",
    "\n",
    "csv_logger = CSVLogger(filename='weights/ssd300_pascal_07+12_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to change the layers to be trained, you must recompile the model. To change it, just set the trainable attribute of a layer either to True or False. In transfer learning usually the added layer on top of the backbone network are trained with the backbone oens frozen, while in a second step, the topmost layer of the original model are defrost and trained along with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McomCaD1bICZ"
   },
   "outputs": [],
   "source": [
    "# set which layers to train. First train the topmost layers, then fine-tune also the convolutional ones.\n",
    "for i, layer in enumerate(model.layers):\n",
    "    layer.trainable = True if i > 22 else False\n",
    "    \n",
    "print(\"COMPILE THE MODEL\\n\")\n",
    "model.compile(optimizer=adam, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkLfDnIvPhuJ"
   },
   "outputs": [],
   "source": [
    "model.load_weights('gdrive/My Drive/ColabFiles/ssc300_4_m_c_ftall_boh.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RH9Q2vrsD_hz"
   },
   "outputs": [],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 70\n",
    "final_epoch     = 100\n",
    "steps_per_epoch = ceil(train_dataset_size/batch_size)\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDXkRf5-vHuc"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6360eifYvC3V"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "predictions = []\n",
    "gts = []\n",
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "# Make a prediction for each test sample feeding the model with the test generator\n",
    "for i in tqdm(range(ceil(test_dataset_size/batch_size))):\n",
    "\n",
    "    batch_x, batch_y = next(test_generator)\n",
    "    imgs.extend(batch_x)\n",
    "    predictions.extend(model.predict(batch_x, batch_size))\n",
    "    gts.extend(batch_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoYPRxygkkA_"
   },
   "outputs": [],
   "source": [
    "imgs = np.array(imgs)\n",
    "pred_res = predictions = np.array(predictions)\n",
    "gts_res = gts = np.array(gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output is split into predictors, it is necessary to reshape and sum it to get the final count for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bk3KRiMQkE59"
   },
   "outputs": [],
   "source": [
    "#pred_res = np.reshape(predictions, (-1,20,len(predictor_sizes)), order='F')\n",
    "#gts_res = np.reshape(gts, (-1,20,len(predictor_sizes)), order='F')\n",
    "\n",
    "print(pred_res.shape, gts_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elwu74qKvNBu"
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "import random\n",
    "\n",
    "#i = 4565 #chairs\n",
    "i = random.randint(0, 4952)\n",
    "\n",
    "imshow(imgs[i])\n",
    "\n",
    "# print(pred_res[i])\n",
    "print('Preds:\\n', np.round(pred_res[i]))\n",
    "print('Truth:\\n', gts_res[i])\n",
    "print((gts_res[i]-np.round(pred_res[i])))\n",
    "# print(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h282a6Yu3gEP"
   },
   "outputs": [],
   "source": [
    "# Clip the eventual negative predictions to 0\n",
    "pred_res = pred_res.clip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuZMufj_vRua"
   },
   "outputs": [],
   "source": [
    "# Compute the evaluation metrics\n",
    "\n",
    "RMSE = np.sqrt(np.mean((pred_res-gts_res)**2))\n",
    "print(\"RMSE: \", RMSE)\n",
    "mRMSE = np.mean(np.sqrt(np.mean((pred_res-gts_res)**2, axis=0)))\n",
    "print(\"mRMSE: \", mRMSE)\n",
    "m_relRMSE = np.mean(np.sqrt(np.mean(((pred_res-gts_res)**2)/(gts_res+1), axis=0)))\n",
    "print(\"m_relRMSE: \", m_relRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilE6pI8ZiaS4"
   },
   "outputs": [],
   "source": [
    "# Obtain the prediction by predictor\n",
    "predictors = np.reshape(predictions, (-1,20,len(predictor_sizes)), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GixUIc01vPde"
   },
   "outputs": [],
   "source": [
    "def pretty_prediction(classes, y_pred, y_true=None, pred_idx=0):\n",
    "    \n",
    "    if y_true is not None:\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            print(\"Prediction {}\".format(i))\n",
    "            for c in range(len(classes)):\n",
    "                if not np.round(y_pred[i,c]) == 0 or not y_true[i,c] == 0:\n",
    "                    print(\"\\t{}: {} {}\".format(classes[c], np.round(y_pred[i,c]), y_true[i,c])) #np.around(predictors[pred_idx,c],1)\n",
    "    else:\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            print(\"Prediction {}\".format(i))\n",
    "            for c in range(len(classes)):\n",
    "                if not y_precontent/d[i,c] == 0:\n",
    "                    print(\"\\t{}: {}\".format(classes[c], y_pred[i,c]))\n",
    "\n",
    "print(\"The first number is the prediction, the second number is the ground truth.\\n\")\n",
    "\n",
    "i = random.randint(0, 4952)\n",
    "# 4469 plane and reflected people\n",
    "# 2106 bottles\n",
    "\n",
    "imshow(imgs[i])\n",
    "print(i)\n",
    "pretty_prediction(classes, pred_res[i:i+1], gts_res[i:i+1], i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zb4EuCGbsrJW"
   },
   "source": [
    "#### Visualize the pixel-wise activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5sFmhdIk8lbo"
   },
   "outputs": [],
   "source": [
    "from lime import lime_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A98C0aXbDmJC"
   },
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "exp = explainer.explain_instance(imgs[i], model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkoDdK1Q-tDG"
   },
   "outputs": [],
   "source": [
    "temp, mask = exp.get_image_and_mask(exp.top_labels[0], positive_only=False, num_features=200)\n",
    "plt.imshow(temp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Amg3pGN-Tz3"
   },
   "outputs": [],
   "source": [
    "for c in exp.top_labels:\n",
    "    print(classes[c])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SSC300.ipynb",
   "provenance": [
    {
     "file_id": "158JozYm6Fd-twoFdRaxXPVHeUOTd7fvn",
     "timestamp": 1558354227632
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
